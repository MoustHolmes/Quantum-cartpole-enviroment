{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoustHolmes/Quantum-cartpole-enviroment/blob/master/DQN_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMdvqRvh6sEq",
        "outputId": "8c1c1f23-e897-4f6b-83d0-2776c2162fb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive')\r\n",
        "!git clone https://github.com/MoustHolmes/Quantum-cartpole-enviroment\r\n",
        "%cd Quantum-cartpole-enviroment\r\n",
        "from Build_DQN_net_keras import build_dqn\r\n",
        "from DQN_keras import DQN_Agent\r\n",
        "from DDQN_keras import DDQN_Agent\r\n",
        "from ReplayBuffer import ReplayBuffer\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Quantum-cartpole-enviroment'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 27 (delta 10), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (27/27), done.\n",
            "/content/Quantum-cartpole-enviroment/Quantum-cartpole-enviroment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U41KWvbR5PTE"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct8iaY_lmclf",
        "outputId": "33fa5ca8-9e19-4c5b-9a94-649dbc729c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\r\n",
        "observation = env.reset()\r\n",
        "\r\n",
        "n_dense1 = 32\r\n",
        "n_dense2 = 64\r\n",
        "\r\n",
        "agent = DQN_Agent(gamma =0.9999, epsilon=1, alpha = 0.0005, input_dims=4,\r\n",
        "              n_dense1 = n_dense1, n_dense2 = n_dense2 ,activation='relu',\r\n",
        "             n_actions=2, mem_size=1000000, batch_size=64, epsilon_end=0.09)\r\n",
        "    \r\n",
        "done =False\r\n",
        "score = 0\r\n",
        "loss_this_game = []\r\n",
        "q_val_this_game = []\r\n",
        "greedy_this_game = []\r\n",
        "obs_this_game = []\r\n",
        "reward_this_game = []\r\n",
        "\r\n",
        "while not done:\r\n",
        "#         env_openAI.render()\r\n",
        "\r\n",
        "    action, q_vals, greedy = agent.choose_action(observation)\r\n",
        "    \r\n",
        "    q_val_this_game.append(q_vals)\r\n",
        "    greedy_this_game.append(greedy)\r\n",
        "    \r\n",
        "    observation_, reward, done, _= env.step(action) #i have deleted info\r\n",
        "    \r\n",
        "    obs_this_game.append(observation_)\r\n",
        "    \r\n",
        "    score += reward\r\n",
        "    reward_this_game.append(reward)\r\n",
        "    \r\n",
        "    agent.remember(observation, action, reward, observation_, done)\r\n",
        "    observation = observation_\r\n",
        "    \r\n",
        "    loss = agent.learn()\r\n",
        "    loss_this_game.append(loss)\r\n",
        "    \r\n",
        "obs_list.append(obs_this_game)\r\n",
        "reward_list.append(reward_this_game)\r\n",
        "q_val_list.append(q_val_this_game)\r\n",
        "greedy_list.append(greedy_this_game)\r\n",
        "loss_list.append(loss_this_game)\r\n",
        "eps_history.append(agent.epsilon)\r\n",
        "scores.append(score)\r\n",
        "\r\n",
        "#     avg_score = np.mean(scores[max(0,i-100):(i+1)])\r\n",
        "#     print('episode ', i,'score %.2f' %score,\r\n",
        "#          'average score %.2f' % avg_score)\r\n",
        "\r\n",
        "if i % 10 == 0 and i > 0:\r\n",
        "        file_name= 'Cart_pole_' +str(n_dense1)+'_'+str(n_dense2)+'_'+str(i)\r\n",
        "        agent.save_model_JSON(file_name)\r\n",
        "        avg_score = np.mean(scores[max(0,i-100):(i+1)])\r\n",
        "        print('episode ', i,'score %.2f' %score,\r\n",
        "              'average score %.2f' % avg_score)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d6f62ca7e764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m agent = DQN_Agent(gamma =0.9999, epsilon=1, alpha = 0.0005, input_dims=4,\n\u001b[1;32m      8\u001b[0m               \u001b[0mn_dense1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_dense1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dense2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_dense2\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m              n_actions=2, mem_size=1000000, batch_size=64, epsilon_end=0.09)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Quantum-cartpole-enviroment/DQN_keras.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, gamma, n_actions, epsilon, batch_size, input_dims, activation, n_dense1, n_dense2, epsilon_dec, epsilon_end, mem_size, fname)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dense2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_dense2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscreate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dense1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dense2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ReplayBuffer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1KEIwbxUy5H"
      },
      "source": [
        "save test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjZ64OXsn4Il"
      },
      "source": [
        "test"
      ]
    }
  ]
}