{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoustHolmes/Quantum-cartpole-enviroment/blob/master/DQN_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMdvqRvh6sEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb132dd-3358-4665-ca61-3462d569641b"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "!git clone https://github.com/MoustHolmes/Quantum-cartpole-enviroment\n",
        "%cd Quantum-cartpole-enviroment\n",
        "from Build_DQN_net_keras import build_dqn\n",
        "from DQN_keras import DQN_Agent\n",
        "from DDQN_keras import DDQN_Agent\n",
        "from ReplayBuffer import ReplayBuffer\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Quantum-cartpole-enviroment'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 33 (delta 14), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n",
            "/content/Quantum-cartpole-enviroment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U41KWvbR5PTE"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct8iaY_lmclf"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "observation = env.reset()\n",
        "\n",
        "n_dense1 = 32\n",
        "n_dense2 = 64\n",
        "\n",
        "agent = DQN_Agent(gamma =0.9999, epsilon=1, alpha = 0.0005, input_dims=4,\n",
        "              n_dense1 = n_dense1, n_dense2 = n_dense2 ,activation='relu',\n",
        "             n_actions=2, mem_size=1000000, batch_size=64, epsilon_end=0.09)\n",
        "    \n",
        "done =False\n",
        "score = 0\n",
        "scores =[]\n",
        "# loss_this_game = []\n",
        "# q_val_this_game = []\n",
        "# greedy_this_game = []\n",
        "# obs_this_game = []\n",
        "# reward_this_game = []\n",
        "\n",
        "while not done:\n",
        "#         env_openAI.render()\n",
        "\n",
        "    action, q_vals, greedy = agent.choose_action(observation)\n",
        "    \n",
        "    # q_val_this_game.append(q_vals)\n",
        "    # greedy_this_game.append(greedy)\n",
        "    \n",
        "    observation_, reward, done, _= env.step(action) #i have deleted info\n",
        "    \n",
        "    # obs_this_game.append(observation_)\n",
        "    \n",
        "    score += reward\n",
        "    # reward_this_game.append(reward)\n",
        "    \n",
        "    agent.remember(observation, action, reward, observation_, done)\n",
        "    observation = observation_\n",
        "    \n",
        "#     loss = agent.learn()\n",
        "#     loss_this_game.append(loss)\n",
        "    \n",
        "# obs_list.append(obs_this_game)\n",
        "# reward_list.append(reward_this_game)\n",
        "# q_val_list.append(q_val_this_game)\n",
        "# greedy_list.append(greedy_this_game)\n",
        "# loss_list.append(loss_this_game)\n",
        "# eps_history.append(agent.epsilon)\n",
        "scores.append(score)\n",
        "\n",
        "#     avg_score = np.mean(scores[max(0,i-100):(i+1)])\n",
        "#     print('episode ', i,'score %.2f' %score,\n",
        "#          'average score %.2f' % avg_score)\n",
        "\n",
        "# if i % 10 == 0 and i > 0:\n",
        "#         file_name= 'Cart_pole_' +str(n_dense1)+'_'+str(n_dense2)+'_'+str(i)\n",
        "#         agent.save_model_JSON(file_name)\n",
        "#         avg_score = np.mean(scores[max(0,i-100):(i+1)])\n",
        "#         print('episode ', i,'score %.2f' %score,\n",
        "#               'average score %.2f' % avg_score)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1KEIwbxUy5H"
      },
      "source": [
        "save test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjZ64OXsn4Il"
      },
      "source": [
        "test"
      ]
    }
  ]
}